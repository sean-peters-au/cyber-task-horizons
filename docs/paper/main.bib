// Alignment Forum/Blog posts
@misc{ngo2023tagi,
  author = {Richard Ngo},
  title = {Clarifying and predicting {AGI}},
  year = {2023},
  howpublished = {\url{https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi}},
  note = {Accessed: 2024-03-21},
  publisher = {LessWrong}
}

// Forecasting
@inproceedings{sevilla2022compute,
  title={Compute trends across three eras of machine learning},
  author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@misc{EpochNotableModels2024,
  title = "Data on Notable {AI} Models",
  author = {{Epoch AI}},
  year = 2024,
  url = {https://epoch.ai/data/notable-ai-models},
  note = "Accessed: 2025-03-04"
}

@misc{owen2024predictable,
      title={How predictable is language model benchmark performance?},
      author={David Owen},
      year={2024},
      eprint={2401.04757},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pimpale2025forecastingfrontierlanguagemodel,
      title={Forecasting Frontier Language Model Agent Capabilities}, 
      author={Govind Pimpale and Axel Højmark and Jérémy Scheurer and Marius Hobbhahn},
      year={2025},
      eprint={2502.15850},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.15850}, 
}

@misc{o3mini,
	author = {OpenAI},
	title = {OpenAI o3-mini},
	howpublished = {\url{https://openai.com/index/openai-o3-mini}},
	year = {2025},
	note = {[Accessed 18-03-2025]},
}

// Pre 2023 Benchmarks
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{paperno2016lambada,
  title={The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1525--1534},
  year={2016}
}

@inproceedings{zellers2019hellaswag,
  title={{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

// Recent Benchmarks
@unpublished{METR_HCAST,
  author = {Rein, David and Becker, Joel and Deng, Amy and Nix, Seraphina and Canal, Chris and O'Connell, Daniel and Arnott, Pip and Bloom, Ryan and Broadley, Thomas and Garcia, Katharyn and Goodrich, Brian and Hasin, Max and Jawhar, Sami and Kinniment, Megan and Kwa, Thomas and Lajko, Aron and Rush, Nate and Sato, Lucas Jun Koba and Von Arx, Sydney and West, Ben and Chan, Lawrence and Barnes, Elizabeth},
  title = {{HCAST}: {H}uman-{C}alibrated {A}utonomy {S}oftware {T}asks},
  year = {2025},
  note = {Forthcoming},
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}

@misc{chowdhury2024SWEbench,
  author       = {Neil Chowdhury and James Aung and Chan Jun Shern and Oliver Jaffe and Dane Sherburn and Giulio Starace and Evan Mays and Rachel Dias and Marwan Aljubeh and Mia Glaese and Carlos E. Jimenez and John Yang and Leyton Ho and Tejal Patwardhan and Kevin Liu and Aleksander Madry},
  title        = {Introducing {SWE}-{b}ench verified},
  howpublished = {\url{https://openai.com/index/introducing-swe-bench-verified/}},
  year         = {2024},
  note         = {Accessed: 2025-02-26}
}

@article{zhang2024cybench,
  title={Cybench: A framework for evaluating cybersecurity capabilities and risks of language models},
  author={Zhang, Andy K and Perry, Neil and Dulepet, Riya and Ji, Joey and Menders, Celeste and Lin, Justin W and Jones, Eliot and Hussein, Gashon and Liu, Samantha and Jasper, Donovan and others},
  journal={arXiv preprint arXiv:2408.08926},
  year={2024}
}

@article{xu2024theagentcompany,
  title={TheAgentCompany: benchmarking llm agents on consequential real world tasks},
  author={Xu, Frank F and Song, Yufan and Li, Boxuan and Tang, Yuxuan and Jain, Kritanjali and Bao, Mengxue and Wang, Zora Z and Zhou, Xuhui and Guo, Zhitong and Cao, Murong and others},
  journal={arXiv preprint arXiv:2412.14161},
  year={2024}
}


@article{phan2025humanity,
  title={Humanity's Last Exam},
  author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and others},
  journal={arXiv preprint arXiv:2501.14249},
  year={2025}
}

@misc{roberts2025zerobench,
      title={{ZeroBench}: An Impossible Visual Benchmark for Contemporary Large Multimodal Models}, 
      author={Jonathan Roberts and Mohammad Reza Taesiri and Ansh Sharma and Akash Gupta and Samuel Roberts and Ioana Croitoru and Simion-Vlad Bogolin and Jialu Tang and Florian Langer and Vyas Raina and Vatsal Raina and Hanyi Xiong and Vishaal Udandarao and Jingyi Lu and Shiyang Chen and Sam Purkis and Tianshuo Yan and Wenye Lin and Gyungin Shin and Qiaochu Yang and Anh Totti Nguyen and David I. Atkinson and Aaditya Baranwal and Alexandru Coca and Mikah Dang and Sebastian Dziadzio and Jakob D. Kunz and Kaiqu Liang and Alexander Lo and Brian Pulfer and Steven Walton and Charig Yang and Kai Han and Samuel Albanie},
      year={2025},
      eprint={2502.09696},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.09696}, 
}

@article{murray2025mapping,
  title={Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert Elicitation},
  author={Murray, Malcolm and Papadatos, Henry and Quarks, Otter and Gimenez, Pierre-Fran{\c{c}}ois and Campos, Simeon},
  journal={arXiv preprint arXiv:2503.04299},
  year={2025}
}

@article{phuong2024evaluating,
  title={Evaluating frontier models for dangerous capabilities},
  author={Phuong, Mary and Aitchison, Matthew and Catt, Elliot and Cogan, Sarah and Kaskasoli, Alexandre and Krakovna, Victoria and Lindner, David and Rahtz, Matthew and Assael, Yannis and Hodkinson, Sarah and others},
  journal={arXiv preprint arXiv:2403.13793},
  year={2024}
}


//Agents
@article{jiang2025aide,
  title={{AIDE}: AI-Driven Exploration in the Space of Code},
  author={Jiang, Zhengyao and Schmidt, Dominik and Srikanth, Dhruv and Xu, Dixing and Kaplan, Ian and Jacenko, Deniss and Wu, Yuxiang},
  journal={arXiv preprint arXiv:2502.13138},
  year={2025}
}

//Control/Mitigations
@article{greenblatt2023ai,
  title={{AI} control: Improving safety despite intentional subversion},
  author={Greenblatt, Ryan and Shlegeris, Buck and Sachan, Kshitij and Roger, Fabien},
  journal={arXiv preprint arXiv:2312.06942},
  year={2023}
}

@article{benton2024sabotage,
  title={Sabotage evaluations for frontier models},
  author={Benton, Joe and Wagner, Misha and Christiansen, Eric and Anil, Cem and Perez, Ethan and Srivastav, Jai and Durmus, Esin and Ganguli, Deep and Kravec, Shauna and Shlegeris, Buck and others},
  journal={arXiv preprint arXiv:2410.21514},
  year={2024}
}

// Psychometrics
@book{baker2001basics,
  title={The basics of item response theory},
  author={Baker, Frank B},
  year={2001},
  publisher={ERIC}
}


// other METR stuff
@article{wijk2024re,
  title={{RE}-{B}ench: Evaluating frontier {AI} {R\&D} capabilities of language model agents against human experts},
  author={Wijk, Hjalmar and Lin, Tao and Becker, Joel and Jawhar, Sami and Parikh, Neev and Broadley, Thomas and Chan, Lawrence and Chen, Michael and Clymer, Josh and Dhyani, Jai and others},
  journal={arXiv preprint arXiv:2411.15114},
  year={2024}
}

// Math
@misc{Roodman2020growthrate,
  title={On the probability distribution of long-term changes in the growth rate of the global economy: An outside view},
  author={David Malin Roodman},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221641799}
}



// Below entries were generated by o3-mini and need to be verified

@misc{amodei2018ai,
  author       = {Amodei, Dario and Hernandez, Danny},
  title        = {AI and Compute},
  year         = {2018},
  note         = {OpenAI blog post, May 16, 2018},
  url          = {https://openai.com/blog/ai-and-compute/}
}

@misc{austin2021program,
  title        = {Program Synthesis with Large Language Models},
  author       = {Austin, Jeremy and Odena, Augustus and others},
  year         = {2021},
  note         = {arXiv preprint, arXiv:2108.07732},
  url          = {https://arxiv.org/abs/2108.07732}
}

@book{boehm1981software,
  title        = {Software Engineering Economics},
  author       = {Boehm, Barry W.},
  year         = {1981},
  publisher    = {Prentice-Hall}
}

@misc{carlsmith2020compute,
  title        = {How Much Computational Power Does It Take to Match the Human Brain?},
  author       = {Carlsmith, Joseph},
  year         = {2020},
  note         = {Open Philanthropy report, August 14, 2020},
  url          = {https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/}
}

@misc{chen2021evaluating,
  title        = {Evaluating Large Language Models Trained on Code},
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  year         = {2021},
  note         = {arXiv preprint, arXiv:2107.03374},
  url          = {https://arxiv.org/abs/2107.03374}
}

@misc{cotra2020forecasting,
  title        = {Draft Report on AI Timelines},
  author       = {Cotra, Ajeya},
  year         = {2020},
  note         = {Draft report on AI timelines; technical report},
  url          = {https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines}
}

@book{de2017handbook,
  title        = {Handbook of Item Response Theory: Models, Applications, and Issues},
  author       = {de Ayala, R. J.},
  year         = {2017},
  publisher    = {Guilford Press}
}

@misc{ho2024algorithmicprogresslanguagemodels,
      title={Algorithmic progress in language models}, 
      author={Anson Ho and Tamay Besiroglu and Ege Erdil and David Owen and Robi Rahman and Zifan Carl Guo and David Atkinson and Neil Thompson and Jaime Sevilla},
      year={2024},
      eprint={2403.05812},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05812}, 
}

@misc{erdil2023algorithmicprogresscomputervision,
      title={Algorithmic progress in computer vision}, 
      author={Ege Erdil and Tamay Besiroglu},
      year={2023},
      eprint={2212.05153},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.05153}, 
}

@article{hendrycks2020measuring,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  journal      = {CoRR},
  volume       = {abs/2009.03300},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.03300},
  eprinttype    = {arXiv},
  eprint       = {2009.03300},
  timestamp    = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-03300.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hendrycks2021measuring,
  title        = {Measuring Coding Challenge Competence with APPS},
  author       = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year         = {2021},
  url          = {https://arxiv.org/abs/2105.09938}
}


@inproceedings{huang2024mlagentbench,
author = {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
title = {{MLAgentBench}: evaluating language agents on machine learning experimentation},
year = {2024},
publisher = {JMLR.org},
abstract = {A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLA-gentBench with 37.5\% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100\% on well-established older datasets to as low as 0\% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {814},
numpages = {39},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{lai2023ds,
  title        = {DS-1000: A Benchmark for Data Science Code Generation},
  author       = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi},
  booktitle    = {Proceedings of the International Conference on Machine Learning (ICML)},
  pages        = {18319--18345},
  year         = {2023},
  publisher    = {PMLR}
}

@article{liu2023agentbench,
  title        = {AgentBench: Evaluating LLMs as Agents},
  author       = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
  year         = {2023},
  journal      = {arXiv preprint arXiv:2308.03688},
  url          = {https://arxiv.org/abs/2308.03688}
}

@misc{maslej2024aiindexreport,
      title={Artificial Intelligence Index Report 2024}, 
      author={Nestor Maslej and Loredana Fattorini and Raymond Perrault and Vanessa Parli and Anka Reuel and Erik Brynjolfsson and John Etchemendy and Katrina Ligett and Terah Lyons and James Manyika and Juan Carlos Niebles and Yoav Shoham and Russell Wald and Jack Clark},
      year={2024},
      eprint={2405.19522},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.19522}, 
}

@inproceedings{mialon2024gaia,
title={{GAIA}: a benchmark for General {AI} Assistants},
author={Gr{\'e}goire Mialon and Cl{\'e}mentine Fourrier and Thomas Wolf and Yann LeCun and Thomas Scialom},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=fibxvahvs3}
}

@article{song2021efficient,
  title={Efficient and robust model benchmarks with item response theory and adaptive testing},
  author={Song, Hao and Flach, Peter},
  year={2021},
  journal={International Journal of Interactive Multimedia and Artificial Intelligence}
}

@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.16789}, 
}

@article{martinezplumed201918item,
title = {Item response theory in AI: Analysing machine learning classifiers at the instance level},
journal = {Artificial Intelligence},
volume = {271},
pages = {18-42},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300220},
author = {Fernando Martínez-Plumed and Ricardo B.C. Prudêncio and Adolfo Martínez-Usó and José Hernández-Orallo},
keywords = {Artificial intelligence evaluation, Item response theory, Machine learning, Instance hardness, Classifier metrics},
abstract = {AI systems are usually evaluated on a range of problem instances and compared to other AI systems that use different strategies. These instances are rarely independent. Machine learning, and supervised learning in particular, is a very good example of this. Given a machine learning model, its behaviour for a single instance cannot be understood in isolation but rather in relation to the rest of the data distribution or dataset. In a dual way, the results of one machine learning model for an instance can be analysed in comparison to other models. While this analysis is relative to a population or distribution of models, it can give much more insight than an isolated analysis. Item response theory (IRT) combines this duality between items and respondents to extract latent variables of the items (such as discrimination or difficulty) and the respondents (such as ability). IRT can be adapted to the analysis of machine learning experiments (and by extension to any other artificial intelligence experiments). In this paper, we see that IRT suits classification tasks perfectly, where instances correspond to items and classifiers correspond to respondents. We perform a series of experiments with a range of datasets and classification methods to fully understand what the IRT parameters such as discrimination, difficulty and guessing mean for classification instances (and their relation to instance hardness measures) and how the estimated classifier ability can be used to compare classifier performance in a different way through classifier characteristic curves.}
}

@book{mcconnell2006software,
  title        = {Software Estimation: Demystifying the Black Art},
  author       = {McConnell, Steve},
  year         = {2006},
  publisher    = {Microsoft Press}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author       = {Srivastava, [Author] and others},
  year         = {2022},
  journal={arXiv preprint arXiv:2206.04615}
}

@inproceedings{wang2018glue,
  title        = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author       = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2018},
  url          = {https://gluebenchmark.com/}
}

@inproceedings{wang2019superglue,
  title        = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Naman and Singh, Amanpreet and Michael, Julian and Bowman, Samuel R.},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) Workshop},
  year         = {2019},
  url          = {https://super.gluebenchmark.com/}
}

@misc{zhang2022ai,
  title        = {The {AI} Index Report 2022},
  author       = {Zhang, Kai and others},
  year         = {2022},
  note         = {Technical report},
  url          = {https://aiindex.stanford.edu/report/}
}

@misc{wang2023mint,
    title={{MINT}: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback},
    author={Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},
    year={2023},
    eprint={2309.10691},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{wei2022emergent,
  title        = {Emergent Abilities of Large Language Models},
  author       = {Wei, Jason and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and others},
  journal      = {Transactions on Machine Learning Research},
  year         = {2022},
  note         = {Also available as arXiv:2206.07682},
  url          = {https://arxiv.org/abs/2206.07682}
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{zielinski_optimize_onboarding,
  author = {Zielinski, Dave},
  title = {How to Optimize Onboarding},
  year = {2019},
  month = {June},
  day = {6},
  url = {https://www.shrm.org/topics-tools/news/hr-magazine/how-to-optimize-onboarding},
  publisher = {Society for Human Resource Management},
  note = {Accessed on March 17, 2025}
}
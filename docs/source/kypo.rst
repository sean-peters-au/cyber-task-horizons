####################################################
Cybersecurity Shell Commands Dataset: Summary and Analysis
####################################################

Introduction
============

This document provides a summary and initial analysis of the processed "Shell Commands Used by Participants of Hands-on Cyber-security Training" dataset (KYPO cyber-range logs). The raw data has been processed by the ``parse_datasets.py`` script to extract individual human session attempts, calculate metrics like duration and command counts, and determine preliminary success flags. The goal is to format this data in a way that is compatible with the METR paper's methodology for evaluating AI agent capabilities.

This summary is generated by the ``summarise_datasets.py`` script, which analyzes the ``data/cybersecurity_human_runs.jsonl`` file.

Overall Dataset Statistics
==========================

The following table provides a high-level overview of the processed dataset.

.. csv-table:: Overall Dataset Statistics
   :file: ../../results/dataset-summaries/overall_summary_stats.csv
   :header-rows: 1
   :widths: auto

**Interpretation:**
`<TODO: Interpret the overall statistics. Key points to consider: total number of usable sessions, overall success rate, and general characteristics of duration and command counts for successful attempts. How does this align with the initial dataset description?>`

Per-Task Family (Training Scenario) Statistics
==============================================

This table breaks down the statistics by each training scenario (task family) present in the dataset.

.. csv-table:: Per-Task Family Statistics
   :file: ../../results/dataset-summaries/per_task_family_stats.csv
   :header-rows: 1
   :widths: auto

**Interpretation:**
`<TODO: Interpret the per-task family statistics. Are there significant differences in success rates, durations, or command counts across different training scenarios? Which scenarios seem easier/harder for humans? Any surprises?>`

Visualizations
==============

1. Distribution of Session Durations (All Valid Sessions)
---------------------------------------------------------

This histogram shows the distribution of ``human_minutes`` for all sessions that passed the initial filtering in ``parse_datasets.py``.

.. image:: ../../results/dataset-summaries/hist_duration_all.png

**Interpretation:**
`<TODO: Describe the shape of the distribution. Are there peaks? Is it skewed? What does this tell us about typical session lengths?>`

2. Distribution of Session Durations (Successful Sessions Only)
---------------------------------------------------------------

This histogram shows the distribution of ``human_minutes`` specifically for sessions marked as successful.

.. image:: ../../results/dataset-summaries/hist_duration_successful.png

**Interpretation:**
`<TODO: Compare this to the distribution for all sessions. Do successful sessions have a different typical length? Is the distribution tighter or more spread out?>`

3. Distribution of Command Counts (All Valid Sessions)
-----------------------------------------------------

This histogram shows the distribution of the total number of commands (``command_count``) executed in each valid session.

.. image:: ../../results/dataset-summaries/hist_command_count_all.png

**Interpretation:**
`<TODO: Describe the shape of this distribution. What are typical command counts? Are there many sessions with very few or very many commands?>`

4. Success Rate by Task Family
------------------------------

This bar chart displays the calculated success rate (percentage) for each task family.

.. image:: ../../results/dataset-summaries/bar_success_rate_per_task_family.png

**Interpretation:**
`<TODO: Which task families have the highest/lowest success rates? Are these rates plausible? How does this correlate with perceived difficulty?>`

5. Median Duration (Successful) by Task Family
----------------------------------------------

This bar chart shows the median ``human_minutes`` for successful sessions within each task family.

.. image:: ../../results/dataset-summaries/bar_median_duration_per_task_family.png

**Interpretation:**
`<TODO: Which successful tasks take the longest/shortest on average (median)? Does this align with the success rates (e.g., do harder tasks take longer even when successful)?>`

6. Command Count vs. Duration (Colored by Success)
--------------------------------------------------

This scatter plot shows the relationship between the total number of commands (``command_count``) and the session duration (``human_minutes``), with points colored by whether the session was marked as successful (``score_binarized``).

.. image:: ../../results/dataset-summaries/scatter_commands_vs_duration.png

**Interpretation:**
`<TODO: Is there a clear relationship between command count and duration? Do successful sessions cluster in a particular area of the plot? Are there any outliers and what might they represent?>`

Conclusion and Next Steps
=========================

This initial summary provides valuable insights into the characteristics of the processed human interaction data from the KYPO cybersecurity dataset. These statistics and visualizations will help inform the subsequent phases of the project, particularly in understanding the human baseline performance against which LLM agents will be benchmarked.

Further refinement of success criteria in ``parse_datasets.py`` may be necessary if the current success rates or patterns seem anomalous after deeper investigation into specific task requirements. 